{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9wqRZsuWbbp"
      },
      "source": [
        "##Data Prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QprJo-jaKpCL"
      },
      "source": [
        "## 2.1 Gather and Explore the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMYCqyAIWd7V"
      },
      "source": [
        "This is for to show what is what !\n",
        "not the actual code or model\n",
        "Get a dataset with (article, summary) pairs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2Cy1VGFw8q8"
      },
      "source": [
        "# Dataset:\n",
        "\n",
        "mounting google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGxNPE4uWqEQ",
        "outputId": "32d0f9ce-b33e-422a-d2c3-cef3cdf69cfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive is already mounted.\n"
          ]
        }
      ],
      "source": [
        "# mounting the drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "if not os.path.ismount('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Drive is mounted.\")\n",
        "else:\n",
        "    print(\"Drive is already mounted.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jOc6rH5x_As"
      },
      "source": [
        "- as seen in the output of this code block dataset include;\n",
        "\n",
        "  only training split is dowloaded to drive.\n",
        "  all training data's size.\n",
        "  each data contains\n",
        "      - {'aid': ..., 'mid': ..., 'ref_abstract': ..., 'abstract': ...}\n",
        "\n",
        "  aid : article id of the actual paper.\n",
        "\n",
        "  mid : reference id of cited or reference paper.\n",
        "\n",
        "  ref_abstract : abstracts of multiple cited or reference papers. Input documents for the model.\n",
        "\n",
        "  abstract : summary of the paper, ground truth for the model.\n",
        "\n",
        "  article id and their abstract summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535,
          "referenced_widgets": [
            "eb77bc338edf484ebb52d6d75599ce94",
            "3ff236d16fe64fc688273489dc424c2f",
            "77b4fb05a920444ebbce437e56887be2",
            "11236dd54d934503aba3fe65c00d0156",
            "0978772c37064cbd8d1e9b15996462b3",
            "dade895c526c455b94de9292d6e7f0e3",
            "d2c959e689bc415c9ffe54ed62639de5",
            "a9350c6b9b73412a97c3322786036c1b",
            "3680a50293404c998d38b52f07db58c4",
            "b440d338250947b1b8ced19c969ad922",
            "b708cc9c29d34823817f64a91db008db",
            "adaa8eda3bec4f2ebd0e3394f3a10a97",
            "bb967a9b8a3a448e8b93027451837618",
            "c7cb1c30b36944f899fad29f4038a387",
            "6a3bc3a3fa2d48a08b101ac5300b1849",
            "1cd3b148e51f410e9130fcf441d9c3e1",
            "41079c6a0f9e40bf84b7f1ae88ae5743",
            "6163fb1190804313abeecdc4454460e5",
            "d586d525c8324dc59ec0da3965849478",
            "3a4b744e9e0446b1b05d8873abb7731c",
            "f56f6a955ad444019c4b8f8f76caf2d0",
            "634a666a10e1486f80c4802912bb4816"
          ]
        },
        "id": "X8-x4i8x9yfs",
        "outputId": "7d6ecfc3-3a69-4495-cd0a-78d20d8ee898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-07e0b5b7d7903f75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-07e0b5b7d7903f75/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb77bc338edf484ebb52d6d75599ce94"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "adaa8eda3bec4f2ebd0e3394f3a10a97"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-07e0b5b7d7903f75/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n",
            "Dataset taken from drive \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "content_of_dataset : {'aid': Value(dtype='string', id=None), 'mid': Value(dtype='string', id=None), 'abstract': Value(dtype='string', id=None), 'ref_abstract': {'mid': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'abstract': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}}\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Total samples: 30369\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "article_id : math9912167\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "reference_paper : 1631980677\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Reference Abstracts (input):\n",
            "  - This note is a sequel to our earlier paper of the same title [4] and describes invariants of rational homology 3-spheres associated to acyclic orthogonal local systems. Our work is in the spirit of\n",
            "the Axelrodâ€“Singer papers [1], generalizes some of their results, and furnishes a new setting for the purely topological implications of their work.\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "  - Recently, Mullins calculated the Casson-Walker invariant of the 2-fold cyclic branched cover of an oriented link in S^3 in terms of its Jones polynomial and its signature, under the assumption that\n",
            "the 2-fold branched cover is a rational homology 3-sphere. Using elementary principles, we provide a similar calculation for the general case. In addition, we calculate the LMO invariant of the p-fold\n",
            "branched cover of twisted knots in S^3 in terms of the Kontsevich integral of the knot.\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Summary\n",
            "  - Author(s): Kuperberg, Greg; Thurston, Dylan P. | Abstract: We give a purely topological definition of the perturbative quantum invariants of links and 3-manifolds associated with Chern-Simons field\n",
            "theory. Our definition is as close as possible to one given by Kontsevich. We will also establish some basic properties of these invariants, in particular that they are universally finite type with\n",
            "respect to algebraically split surgery and with respect to Torelli surgery. Torelli surgery is a mutual generalization of blink surgery of Garoufalidis and Levine and clasper surgery of Habiro.\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import textwrap\n",
        "!pip install datasets==2.0 --quiet\n",
        "import textwrap\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "save_dir = \"/content/drive/MyDrive/00_Bireysel/02_dersler/nlp/project/my_paper/resources\"\n",
        "save_path = os.path.join(save_dir, \"multi_science_dataset.json\")\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "  print(\"Dataset downloading...\")\n",
        "  multi_xscience = load_dataset(\"bigbio/multi_xscience\")\n",
        "  multi_xscience['train'].to_json(save_path)\n",
        "  multi_xscience = multi_xscience[multi_xscience[\"train\"]]\n",
        "  print(\"Dataset downloaded and saved\")\n",
        "else:\n",
        "  multi_xscience_train = load_dataset(\"json\", data_files=save_path, split=\"train\")\n",
        "  print(\"Dataset taken from drive \")\n",
        "\n",
        "\n",
        "print(f'-'*200)\n",
        "content_of_dataset = multi_xscience_train.features\n",
        "print(f\"content_of_dataset : {content_of_dataset}\")\n",
        "print(f'-'*200)\n",
        "\n",
        "#for split in multi_xscience:\n",
        "#    print(f\"Split: {split}\")\n",
        "data = multi_xscience_train\n",
        "num_samples = len(data)\n",
        "print(f\"Total samples: {num_samples}\")\n",
        "\n",
        "print(f'-'*200)\n",
        "random_index = 0\n",
        "example = multi_xscience_train[random_index]\n",
        "article_id = example['aid']\n",
        "print(f\"article_id : {article_id }\")\n",
        "print(f'-'*200)\n",
        "\n",
        "reference_paper = example[\"mid\"]\n",
        "print(f\"reference_paper : {reference_paper}\")\n",
        "print(f'-'*200)\n",
        "\n",
        "# looping through abstract references.\n",
        "print(\"Reference Abstracts (input):\")\n",
        "for ref in example[\"ref_abstract\"][\"abstract\"]:\n",
        "  print(\"  - \" + textwrap.fill(ref, width=200))\n",
        "  print(f'-'*200)\n",
        "\n",
        "print(\"Summary\")\n",
        "print(\"  - \" + textwrap.fill(example[\"abstract\"], width=200))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw5rC8QG92cY"
      },
      "source": [
        "# some visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR4SgWE5GkP_"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qccs2OW962C"
      },
      "source": [
        "\n",
        "- vocabulary\n",
        "  combining all words from source data to create a vocabulary. lowercase all letters and splitting the words.\n",
        "  using word2idx each word is mapped to unique ID.\n",
        "- what is tokenization :\n",
        "  \n",
        "    converting raw text into tokens, after that each token is mapped to an id so that it can be used as input for models.\n",
        "\n",
        "- what are the parameters\n",
        "      max_legth\n",
        "    is the maximum number of tokens in encoded sentence.\n",
        "      <pad>\n",
        "    in case of shorter sentence padding ensures uniform length\n",
        "      <unk>\n",
        "    representing words that are  not on the vocablary.\n",
        "      truncate\n",
        "    cutting the sentence if it exceeds the max length.\n",
        "\n",
        "- input for tokenization and output of tokenization :\n",
        "\n",
        "  normal source text becomes a list of tokens which include their unique id.\n",
        "  If these tokens are decoded, original words can be extracted with paddings(< pad >)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy-upga597dU"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "class SimpleTokenizer:\n",
        "    full_size = 269401\n",
        "    def __init__(self, data, vocab_limit=25000):  #to add a limit !\n",
        "                                        #269401\n",
        "        self.data = data\n",
        "        self.vocab_limit = vocab_limit\n",
        "        self.build_vocab()\n",
        "        print(f\"vocab size : {vocab_limit}]\")\n",
        "\n",
        "\n",
        "    def build_vocab(self):\n",
        "        word_counter = Counter()\n",
        "        for item in self.data:\n",
        "            for text in item[\"ref_abstract\"][\"abstract\"]:\n",
        "                word_counter.update(text.lower().split())\n",
        "            word_counter.update(item[\"abstract\"].lower().split())\n",
        "\n",
        "        most_common_words = word_counter.most_common(self.vocab_limit)\n",
        "        self.word2idx = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "        for idx, (word, _) in enumerate(most_common_words, start=2):\n",
        "            self.word2idx[word] = idx\n",
        "\n",
        "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
        "\n",
        "    def encode(self, text, max_length):\n",
        "        tokens = text.lower().split()\n",
        "        token_ids = [\n",
        "            self.word2idx.get(word, self.word2idx[\"<unk>\"]) for word in tokens\n",
        "        ]\n",
        "        if len(token_ids) < max_length:\n",
        "            token_ids += [self.word2idx[\"<pad>\"]] * (max_length - len(token_ids))\n",
        "        else:\n",
        "            token_ids = token_ids[:max_length]\n",
        "        return token_ids\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        return \" \".join(self.idx2word.get(idx, \"<unk>\") for idx in token_ids)\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return len(self.word2idx)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e44y4-hUGK5Q"
      },
      "source": [
        "## dummy showcase\n",
        "\n",
        "using the dummy dataset we can observe the input and output of this process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iM-EeihT-Emo",
        "outputId": "57f308db-a86a-4f44-9e3b-4ad2b13b7d67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This note is a sequel to our earlier paper of the same title [4] and describes invariants of rational homology 3-spheres associated to acyclic orthogonal local systems. Our work is in the spirit of\n",
            "the Axelrodâ€“Singer papers [1], generalizes some of their results, and furnishes a new setting for the purely topological implications of their work. Recently, Mullins calculated the Casson-Walker\n",
            "invariant of the 2-fold cyclic branched cover of an oriented link in S^3 in terms of its Jones polynomial and its signature, under the assumption that the 2-fold branched cover is a rational homology\n",
            "3-sphere. Using elementary principles, we provide a similar calculation for the general case. In addition, we calculate the LMO invariant of the p-fold branched cover of twisted knots in S^3 in terms\n",
            "of the Kontsevich integral of the knot.\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "vocab size : 25000]\n",
            "encoded : [34, 35, 7, 6, 36, 8, 11, 37, 38, 2, 3, 39, 40, 41, 4, 42, 15, 2, 16, 17, 43, 18, 8, 44, 45, 46, 47, 11, 48, 7, 5, 3, 49, 2, 3, 50, 51, 52, 53, 19, 2, 20, 54, 4, 55, 6, 56, 57, 21, 3, 22, 23, 58, 2, 20, 59, 60, 61, 62, 3, 63, 24, 2, 3, 25, 64, 12, 13, 2, 65, 66, 67, 5, 26, 5, 27, 2, 28, 68, 69, 4, 28, 70, 71, 3, 72, 29, 3, 25, 12, 13, 7, 6, 16, 17, 73, 74, 75, 76, 9, 77, 6, 78, 79, 21, 3, 80, 81, 5, 82, 9, 83, 3, 84, 24, 2, 3, 85, 12, 13, 2, 86, 87, 5, 26, 5, 27, 2, 3, 88, 89, 2, 3, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "this note is a sequel to our earlier paper of the same title [4] and describes invariants of rational homology 3-spheres associated to acyclic orthogonal local systems. our work is in the spirit of\n",
            "the axelrodâ€“singer papers [1], generalizes some of their results, and furnishes a new setting for the purely topological implications of their work. recently, mullins calculated the casson-walker\n",
            "invariant of the 2-fold cyclic branched cover of an oriented link in s^3 in terms of its jones polynomial and its signature, under the assumption that the 2-fold branched cover is a rational homology\n",
            "3-sphere. using elementary principles, we provide a similar calculation for the general case. in addition, we calculate the lmo invariant of the p-fold branched cover of twisted knots in s^3 in terms\n",
            "of the kontsevich integral of the knot. <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
          ]
        }
      ],
      "source": [
        "dummy_train_data = [multi_xscience_train[0]]  # Only 1 sample\n",
        "\n",
        "source_text = \" \".join(dummy_train_data[0][\"ref_abstract\"][\"abstract\"])\n",
        "print(textwrap.fill(source_text, width=200))\n",
        "print(\"-\" * 200)\n",
        "\n",
        "simple_tokenizer = SimpleTokenizer(dummy_train_data)\n",
        "encoded = simple_tokenizer.encode(source_text, max_length = 153)\n",
        "print(f\"encoded : {encoded}\")\n",
        "print(\"-\" * 200)\n",
        "\n",
        "decoded = simple_tokenizer.decode(encoded)\n",
        "print(textwrap.fill(decoded, width=200))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EuvPYY0GvuF"
      },
      "source": [
        "#Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zIHCZJmHxxv"
      },
      "source": [
        "- Encoders :\n",
        "\n",
        "  encoders takes sequence of tokens and turn them into hidden representaion. In this case using embedding layer and Long Short-Term Memory (LSTM) layer.\n",
        "\n",
        "- Parameters :\n",
        "   \n",
        "      vocab_size  \n",
        "  is obtained from the tokenizer class\n",
        "      embed_dim\n",
        "  how many numbers represent each wordÃ§\n",
        "      hidden_dim\n",
        "  size of LSTM's memory\n",
        "      num_layers\n",
        "  number of stacked LSTM layers\n",
        "    \n",
        "\n",
        "\n",
        "- pipeline itself\n",
        "      \n",
        "  1 - embedding_layer\n",
        "  \n",
        "  input : token id shape is (1, 153(max_legth))\n",
        "      embed_dim = 128\n",
        "  output : embedded vectors with shape (1, 153, 128)\n",
        "  1 : number of sequence.\n",
        "  153 : number of tokens\n",
        "  128 : each word become dimensional vector.\n",
        "\n",
        "  2 - LSTM Layer\n",
        "\n",
        "  input : (1, 153, 128)\n",
        "  for each word LSTM outpur (1, 153, 256) 256-dim vector\n",
        "  with two LSTM layers (2, 153, 256)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  Output *****\n",
        "\n",
        "\n",
        "      outputs\n",
        "  LSTM output at each time step\n",
        "\n",
        "  embeddings are located here.\n",
        "\n",
        "      hidden\n",
        "  final hidden state of each LSTM layer. (initializing decoder)\n",
        "    a kind of memory of LSTM\n",
        "      cell\n",
        "  \n",
        "  internal memory of each LSTM(initializing decoder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_2t7-QaGwT3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "    self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
        "    print(f\"self.embedding : {self.embedding}\") # DB - Size of your vocab (136 unique tokens: words + <pad> + <unk>)\n",
        "    print(f\"self.lstm : {self.lstm}\")           # DB - Each word will be represented as a vector of 128 numbers (dense, not one-hot).\n",
        "  def forward(self, input_ids):\n",
        "    embedded = self.embedding(input_ids)               # (batch_size, seq_len, embed_dim)\n",
        "    outputs, (hidden, cell) = self.lstm(embedded)       # (batch_size, seq_len, hidden_dim)\n",
        "    return outputs, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8psJ1YMzGzVx",
        "outputId": "045c12c6-f487-4b40-b31e-6a5586e083b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This note is a sequel to our earlier paper of the same title [4] and describes invariants of rational homology 3-spheres associated to acyclic orthogonal local systems. Our work is in the spirit of\n",
            "the Axelrodâ€“Singer papers [1], generalizes some of their results, and furnishes a new setting for the purely topological implications of their work. Recently, Mullins calculated the Casson-Walker\n",
            "invariant of the 2-fold cyclic branched cover of an oriented link in S^3 in terms of its Jones polynomial and its signature, under the assumption that the 2-fold branched cover is a rational homology\n",
            "3-sphere. Using elementary principles, we provide a similar calculation for the general case. In addition, we calculate the LMO invariant of the p-fold branched cover of twisted knots in S^3 in terms\n",
            "of the Kontsevich integral of the knot.\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "vocab size : 25000]\n",
            "vocab_size : 136\n",
            "self.embedding : Embedding(136, 128)\n",
            "self.lstm : LSTM(128, 256, num_layers=2, batch_first=True)\n",
            "Input tensor shape (to Encoder) : torch.Size([1, 153])\n",
            "Outputs shape (from Encoder)    : torch.Size([1, 153, 256])\n",
            "Hidden state shape              : torch.Size([2, 1, 256])\n",
            "Cell state shape                : torch.Size([2, 1, 256])\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "tensor([[-0.0113, -0.0293,  0.0184,  0.0029,  0.0232,  0.0155, -0.0004,  0.0147,\n",
            "          0.0060, -0.0119],\n",
            "        [ 0.0074, -0.0370,  0.0291,  0.0217,  0.0358,  0.0004, -0.0162,  0.0387,\n",
            "         -0.0028, -0.0303],\n",
            "        [ 0.0288, -0.0199,  0.0404,  0.0139,  0.0507,  0.0217, -0.0346,  0.0343,\n",
            "          0.0381, -0.0138],\n",
            "        [ 0.0462,  0.0156,  0.0484,  0.0129,  0.0659,  0.0166, -0.0458,  0.0355,\n",
            "          0.0453, -0.0045],\n",
            "        [ 0.0498,  0.0514,  0.0639,  0.0139,  0.0677,  0.0168, -0.0671,  0.0239,\n",
            "          0.0581,  0.0060]], grad_fn=<SliceBackward0>)\n",
            "tensor([[ 0.3263, -0.0573,  0.1590,  0.0083,  0.0100, -0.0235,  0.0695,  0.0941,\n",
            "          0.0931, -0.0745],\n",
            "        [-0.0027,  0.0240,  0.0537, -0.0100,  0.0012, -0.0395, -0.0065, -0.0580,\n",
            "          0.0988, -0.0746]], grad_fn=<SliceBackward0>)\n",
            "tensor([[ 1.4967, -0.0846,  0.4230,  0.0156,  0.0203, -0.0594,  0.1715,  0.1725,\n",
            "          0.1816, -0.1145],\n",
            "        [-0.0052,  0.0473,  0.1042, -0.0222,  0.0026, -0.0797, -0.0128, -0.1182,\n",
            "          0.1843, -0.1632]], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ],
      "source": [
        "## DUMMY USECASE !\n",
        "import torch\n",
        "import textwrap\n",
        "\n",
        "dummy_train_data = [multi_xscience_train[0]]\n",
        "\n",
        "# creating a dummy dataset\n",
        "source_text = \" \".join(dummy_train_data[0][\"ref_abstract\"][\"abstract\"])\n",
        "print(textwrap.fill(source_text, width=200))\n",
        "print(\"-\" * 200)\n",
        "\n",
        "# using tokenizer\n",
        "simple_tokenizer = SimpleTokenizer(dummy_train_data)\n",
        "\n",
        "encoded = simple_tokenizer.encode(source_text, max_length=153)\n",
        "\n",
        "decoded = simple_tokenizer.decode(encoded)\n",
        "input_tensor = torch.tensor([encoded], dtype=torch.long)  # batch_size = 1\n",
        "\n",
        "# Initializing Encoder\n",
        "vocab_size = simple_tokenizer.get_vocab_size()\n",
        "print(f\"vocab_size : {vocab_size}\")\n",
        "embed_dim = 128\n",
        "hidden_dim = 256\n",
        "num_layers = 2\n",
        "\n",
        "encoder = Encoder(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=embed_dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    num_layers=num_layers\n",
        ")\n",
        "\n",
        "# pass through encoder !\n",
        "outputs, hidden, cell = encoder(input_tensor)\n",
        "\n",
        "# 9. Print what went into encoder and what came out\n",
        "print(f\"Input tensor shape (to Encoder) : {input_tensor.shape}\")     # (batch_size, seq_len)\n",
        "print(f\"Outputs shape (from Encoder)    : {outputs.shape}\")          # (batch_size, seq_len, hidden_dim)\n",
        "print(f\"Hidden state shape              : {hidden.shape}\")           # (num_layers, batch_size, hidden_dim)\n",
        "print(f\"Cell state shape                : {cell.shape}\")             # (num_layers, batch_size, hidden_dim)\n",
        "print(\"-\" * 200)\n",
        "print(outputs[0, :5, :10])  # first 5 tokens, first 10 hidden dimensions\n",
        "print(hidden[:, 0, :10])    # hidden states (all layers), first 10 hidden dims\n",
        "print(cell[:, 0, :10])      # cell states (all layers), first 10 hidden dims\n",
        "\n",
        "# Word \"the\" â†’ [0.12, -0.44, 0.78, ..., 0.01] (a vector of 128 numbers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBepFDGMXI_U"
      },
      "source": [
        "#Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJYNOTziXMY8"
      },
      "source": [
        "In terms of structure decoder and encoder are similar.\n",
        "\n",
        "both use embedding layyers to convert tokens to vectors\n",
        "\n",
        "use LSTM to process sequence of data\n",
        "\n",
        "start with <sos> token then continue one by one.\n",
        "It outputs a sequence of token prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqI8lGV6XLe2"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "    self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
        "    self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "  def forward(self, input_ids, hidden, cell):\n",
        "    embedded = self.embedding(input_ids)  # (batch_size, seq_len, embed_dim)\n",
        "    outputs, (hidden, cell) = self.lstm(embedded, (hidden, cell))  # Use the previous hidden and cell states\n",
        "    predictions = self.fc_out(outputs)  # (batch_size, seq_len, vocab_size)\n",
        "    return predictions, hidden, cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2wjk4iec26H"
      },
      "source": [
        "# seq2seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpyK_t8FdoyN"
      },
      "source": [
        "##\n",
        "\n",
        "encoder converts input to memory\n",
        "\n",
        "decoder create new output from memory(encoder)\n",
        "\n",
        "\n",
        "seq2seq handles one full forward pass!\n",
        "\n",
        "      def forward(self, src, trg)\n",
        "\n",
        "  forward is the actual method, responsible for the pass through network.\n",
        "  \n",
        "      src\n",
        "  represent the input ensor with shape of batch size and src_seq_len\n",
        "\n",
        "      trg\n",
        "  ground truth output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r_hXSmWf-d2"
      },
      "source": [
        "##flow\n",
        "\n",
        "output of forward method's one pass  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVaoYJitX5Dy"
      },
      "outputs": [],
      "source": [
        "# actual seq2seq class !\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, device):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "\n",
        "  def forward(self, src, trg):\n",
        "    batch_size = src.shape[0]\n",
        "    trg_len = trg.shape[1]\n",
        "    vocab_size = self.decoder.fc_out.out_features\n",
        "\n",
        "    outputs = torch.zeros(batch_size, trg_len, vocab_size).to(self.device)\n",
        "\n",
        "    encoder_outputs, hidden, cell = self.encoder(src)\n",
        "\n",
        "    input_token = trg[:, 0].unsqueeze(1)\n",
        "    for t in range(1, trg_len):\n",
        "        output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
        "        outputs[:, t] = output.squeeze(1)\n",
        "\n",
        "        top_tok = output.argmax(2)\n",
        "        input_token = top_tok\n",
        "\n",
        "    return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzmtiTgmgikd"
      },
      "source": [
        "# Dataset Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zniQHTtW0lK"
      },
      "source": [
        "actual dataset class to better use our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azTeuqesgkhb"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self, data, tokenizer, max_input_len=256, max_target_len=128):\n",
        "    self.data = data\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_input_len = max_input_len\n",
        "    self.max_target_len = max_target_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = self.data[idx]\n",
        "\n",
        "    # Prepare source and target texts\n",
        "    source_text = \" \".join(item[\"ref_abstract\"][\"abstract\"])\n",
        "    target_text = item[\"abstract\"]\n",
        "\n",
        "    # Tokenize\n",
        "    source_ids = self.tokenizer.encode(source_text, self.max_input_len)\n",
        "    target_ids = self.tokenizer.encode(target_text, self.max_target_len)\n",
        "\n",
        "    return {\n",
        "      \"input_ids\": torch.tensor(source_ids, dtype=torch.long),\n",
        "      \"labels\": torch.tensor(target_ids, dtype=torch.long)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye5YndLMgYYg"
      },
      "source": [
        "# initilaizing all\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxaBf7VtgaFu",
        "outputId": "29851f7d-c9f8-4567-a502-7a4aadc9a46b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data set ength : 30369\n",
            "vocab size : 25000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "train_dataset = MyDataset(multi_xscience_train, simple_tokenizer)\n",
        "\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "\n",
        "print(f\"data set ength : {len(train_dataset)}\")\n",
        "reduced_data = multi_xscience_train.select(range(len(train_dataset)))\n",
        "\n",
        "# Create tokenizer\n",
        "simple_tokenizer = SimpleTokenizer(reduced_data)\n",
        "\n",
        "# Wrap with dataset\n",
        "train_dataset = MyDataset(reduced_data, simple_tokenizer)\n",
        "\n",
        "# Create DataLoader\n",
        "# 128/512/256\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers = 16, pin_memory = True)\n",
        "_, val_data = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "val_loader = DataLoader(val_data, batch_size=32)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smsG5AOSYm4e"
      },
      "source": [
        "# training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQUQpJTdZddE"
      },
      "source": [
        "      device\n",
        "\n",
        "  if available choose a CPU cuda else runs on CPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O_mLnA4Zp9k"
      },
      "source": [
        "  These are hyperparameters that effect model's performance.\n",
        "\n",
        "      vocab_size\n",
        "      embed_dim = 256\n",
        "\n",
        "  - it is how many numbers represent each word(token)\n",
        "  - higher dimension meaning a richer representation with more memory usage.\n",
        "  - much higher dimensions is not neccesary in small datasets.\n",
        "\n",
        "\n",
        "      hidden_dim = 512\n",
        "\n",
        "  \n",
        "  - size of the hidden state in LSTM.\n",
        "  - it is responsible for how much info LSTM can store and process.\n",
        "  - bigger memory meaning better \"thinking\", yet can be expensive computationaly.\n",
        "  - higher risk of overfitting in small datasets.     \n",
        "      \n",
        "\n",
        "      num_layers = 2\n",
        "\n",
        "- number of stacked LSTM layers.\n",
        "- more layer a better model that can comprehend more complex patterns.\n",
        "- too many layers create diffuculty in training.\n",
        "  \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i76e7gNXc5bZ"
      },
      "source": [
        "  setting up values for encoder, decoder and wrap them into seq2seq model.\n",
        "\n",
        "      encoder = Encoder(vocab_size, embed_dim, hidden_dim, num_layers)\n",
        "      decoder = Decoder(vocab_size, embed_dim, hidden_dim, num_layers)\n",
        "      model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "----------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31J8b7PddEAF"
      },
      "source": [
        "based on how our model learns we can adjust these values.\n",
        "\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "optimizer :\n",
        "  \n",
        "  - controls how the model updates its weights, according to the loss.\n",
        "\n",
        "ADAM - Adaptive Moment Estimation :\n",
        "  - chosen according to popularity amoung ML applications.\n",
        "\n",
        "  - using momentum it updates learning rates adaptively.\n",
        "\n",
        "learning rate :\n",
        "  - chosen according to safe value for ADAM optimizer.  \n",
        "  - important hyperparameter.\n",
        "  - controls how big each update is to the weights.\n",
        "   \n",
        "   *small learning rate = > model learns slow\n",
        "   \n",
        "   *big model skip important details.\n",
        "- - - - - -- - - - - - -\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIv6cpMWfjX8"
      },
      "source": [
        "loss function :\n",
        "\n",
        "- telling the model, how wrong it is. Since we are predicting tokens from vocablary CrossEntropyLoss is good choice.\n",
        "\n",
        "- <pad> tokens are ignored, since they represent padding, not actually meaningfull words.\n",
        "\n",
        "      criterion = nn.CrossEntropyLoss(ignore_index=simple_tokenizer.word2idx[\"<pad>\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPSeWETnfqVV"
      },
      "source": [
        "Epochs :\n",
        "\n",
        "this is where the actual loop is located.\n",
        "1 epoch meaning a full pass through the dataset.\n",
        "      \n",
        "      num_of_epochs\n",
        "\n",
        "decide how many times dataset will go through the model during training.\n",
        "\n",
        "-----------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC_BKx8bgkjU"
      },
      "source": [
        "      for i, batch in enumerate(train_loader):\n",
        "        src = batch[\"input_ids\"].to(device)\n",
        "        trg = batch[\"labels\"].to(device)\n",
        "  \n",
        "iterating in training data in mini-batches.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      input = batch[\"input_ids\"].to(device)\n",
        "      target = batch[\"labels\"].to(device)\n",
        "\n",
        "\n",
        "- src : input, target : target summary as tensors.\n",
        "- these tensors are moved to device(GPU or CPU)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "- clears ol gradients from last step. precent gradient accumulation.\n",
        "\n",
        "\n",
        "      \n",
        "      output = model(inputs, target)\n",
        "- passes the source and target througgh seq2seq model\n",
        "output : is a tensor of predicted word probabilities, also called logits\n",
        "\n",
        "      output_dim = output.shape[-1]    \n",
        "      output = output[:, 1:].reshape(-1, output_dim)\n",
        "      target = target[:, 1:].reshape(-1)\n",
        "\n",
        "- skipping the sos token\n",
        "- flattening predictions and targets for CrossEntropyLoss\n",
        "\n",
        "output - shape => [batch_size * seq_len, vocab_size]\n",
        "\n",
        "target - shape => [batch_size * seq_len]\n",
        "\n",
        "--------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Loy0hLf0k0qU"
      },
      "source": [
        "      loss = criterion(output, target)\n",
        "\n",
        "- comparing model prediction and actual token\n",
        "- how wrong the prediction is\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIPfmqFElVaf"
      },
      "source": [
        "      loss.backward()\n",
        "- backpropagation step, calculate gradients for all parameters and observe how much each is contributed to the loss.\n",
        "- how much we need to adjust each weight for reduced loss.\n",
        "----------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvFSc27Xltz3"
      },
      "source": [
        "      optimizer.step()\n",
        "- updating the model weights according yo previous calculated(loss.backward).\n",
        "\n",
        "- optimizer uses gradients and learning rate to adjust weights.\n",
        "\n",
        "--------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwdgV-YWmFpC"
      },
      "source": [
        "Every 10 batches, the batch number is displayed.\n",
        "the average loss is calculated to observe training performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh3Hpufd2W9c"
      },
      "source": [
        "# training code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "jWQROvqLHR9h",
        "outputId": "976237b0-9720-4212-c083-604d03522fd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.29.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkonuralp-dalkilinc\u001b[0m (\u001b[33mkonuralp-dalkilinc-i-k-niversitesi\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250531_070010-w5gj005i</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/konuralp-dalkilinc-i-k-niversitesi/seq2seq_training/runs/w5gj005i' target=\"_blank\">rev8_run</a></strong> to <a href='https://wandb.ai/konuralp-dalkilinc-i-k-niversitesi/seq2seq_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/konuralp-dalkilinc-i-k-niversitesi/seq2seq_training' target=\"_blank\">https://wandb.ai/konuralp-dalkilinc-i-k-niversitesi/seq2seq_training</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/konuralp-dalkilinc-i-k-niversitesi/seq2seq_training/runs/w5gj005i' target=\"_blank\">https://wandb.ai/konuralp-dalkilinc-i-k-niversitesi/seq2seq_training/runs/w5gj005i</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "\n",
        "wandb.login(key=\"be3adc9c65fc0519d65f2c1b73b052672597ebcb\")\n",
        "\n",
        "wandb.init(project=\"seq2seq_training\", name=\"rev8_run\")\n",
        "\n",
        "config = wandb.config\n",
        "config.vocab_limit = 25000\n",
        "config.embed_dim = 256\n",
        "config.hidden_dim = 128\n",
        "config.num_layers = 2\n",
        "config.learning_rate = 0.001\n",
        "config.epochs = 5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-T7eAHiYoD7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Step 5 - Training\n",
        "#Â 29_05_2025 - adding validation tracker\n",
        "# use wandb\n",
        "train = False\n",
        "patience=3\n",
        "if train:\n",
        "\n",
        "  device = torch.device(\"cuda\")\n",
        "\n",
        "  vocab_size = simple_tokenizer.get_vocab_size()\n",
        "  embed_dim = 256    #\n",
        "  hidden_dim = 128   # hidden_dimension 128 !\n",
        "  num_layers = 2\n",
        "\n",
        "  encoder = Encoder(vocab_size, embed_dim, hidden_dim, num_layers)\n",
        "  decoder = Decoder(vocab_size, embed_dim, hidden_dim, num_layers)\n",
        "  model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "  wandb.watch(model, log=\"all\")\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "  criterion = nn.CrossEntropyLoss(ignore_index=simple_tokenizer.word2idx[\"<pad>\"])\n",
        "\n",
        "  num_of_epochs = 15\n",
        "  best_val_loss = float('inf')\n",
        "  early_stopping_counter = 0\n",
        "  save_path = \"/content/drive/MyDrive/00_Bireysel/02_dersler/nlp/project/my_paper\"\n",
        "  os.makedirs(save_path, exist_ok=True)\n",
        "  number_of_revision = 1\n",
        "  save_name = f\"{number_of_revision}_seq2seq_model_8.pth\"\n",
        " #Â for each epoch add validation !\n",
        " # stop when validation is early stoping validation !\n",
        "  for epoch in range(num_of_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "      inputs = batch[\"input_ids\"].to(device)\n",
        "      target = batch[\"labels\"].to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      output = model(inputs, target)\n",
        "\n",
        "      output_dim = output.shape[-1]\n",
        "      output = output[:, 1:].reshape(-1, output_dim)\n",
        "      target = target[:, 1:].reshape(-1)\n",
        "\n",
        "      loss = criterion(output, target)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if i % 10 == 0:\n",
        "        print(f\"  Batch {i}/{len(train_loader)}\")\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_of_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "#    print(f\"Epoch [{epoch+1}/{num_of_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "   # VALIDATION BLOCK ***\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "      for val_batch in val_loader:\n",
        "        inputs = val_batch[\"input_ids\"].to(device)\n",
        "        target = val_batch[\"labels\"].to(device)\n",
        "\n",
        "        output = model(inputs, target)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:].reshape(-1, output_dim)\n",
        "        target = target[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_of_epochs}], Val Loss: {val_loss:.4f}\")\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train_loss\": avg_loss,\n",
        "        \"val_loss\": val_loss\n",
        "    })\n",
        "    # stopper logic\n",
        "    if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      early_stopping_counter = 0\n",
        "\n",
        "      # Save best model\n",
        "      torch.save(model.state_dict(), os.path.join(save_path, \"best_model.pth\"))\n",
        "      print(\" Best model saved \")\n",
        "    else:\n",
        "      early_stopping_counter += 1\n",
        "      print(f\" EarlyStopping Counter: {early_stopping_counter}/{patience}\")\n",
        "\n",
        "      if early_stopping_counter >= patience:\n",
        "        print(\"stopping early\")\n",
        "        break\n",
        "\n",
        "  number_of_revision = 1\n",
        "\n",
        "  torch.save(model.state_dict(), os.path.join(save_path, save_name))\n",
        "  wandb.save(os.path.join(save_path, save_name))\n",
        "  wandb.finish()\n",
        "\n",
        "\n",
        "  import json\n",
        "  with open(os.path.join(save_path, \"vocab_8.json\"), \"w\") as f:\n",
        "      json.dump(simple_tokenizer.word2idx, f)\n",
        "\n",
        "  print(f\"Model and tokenizer saved to {save_path}\")\n",
        "else:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpUtfKLvm4xr"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAUS6Jow15x0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dda9e61-9055-450b-ea7f-94dfcf0c8153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size : 25002\n",
            "self.embedding : Embedding(25002, 256)\n",
            "self.lstm : LSTM(256, 128, num_layers=2, batch_first=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(25002, 256)\n",
              "    (lstm): LSTM(256, 128, num_layers=2, batch_first=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(25002, 256)\n",
              "    (lstm): LSTM(256, 128, num_layers=2, batch_first=True)\n",
              "    (fc_out): Linear(in_features=128, out_features=25002, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import json\n",
        "with open(\"/content/drive/MyDrive/00_Bireysel/02_dersler/nlp/project/my_paper/vocab.json\", \"r\") as f:\n",
        "    word2idx = json.load(f)\n",
        "\n",
        "vocab_size = simple_tokenizer.get_vocab_size()\n",
        "# These parameters need to change according to model itself.\n",
        "print(f\"vocab_size : {vocab_size}\")\n",
        "embed_dim = 256    #\n",
        "hidden_dim = 128   #\n",
        "num_layers = 2\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "encoder = Encoder(vocab_size, embed_dim, hidden_dim, num_layers)\n",
        "decoder = Decoder(vocab_size, embed_dim, hidden_dim, num_layers)\n",
        "model = Seq2Seq(encoder, decoder, device)\n",
        "\n",
        "# Loading the model\n",
        "# using cpu just to predict.\n",
        "model.load_state_dict(torch.load(\n",
        "    \"/content/drive/MyDrive/00_Bireysel/02_dersler/nlp/project/my_paper/best_model.pth\",\n",
        "    map_location=\"cpu\"\n",
        "))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5bBI_hMQxqN"
      },
      "outputs": [],
      "source": [
        "def predict(model, tokenizer, input_text, max_len=64):\n",
        "  model.eval()\n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "  input_ids = tokenizer.encode(input_text, max_length=128)\n",
        "  input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "  with torch.no_grad():\n",
        "    _, hidden, cell = model.encoder(input_tensor)\n",
        "\n",
        "  decoder_input = torch.tensor([[tokenizer.word2idx.get(\"<pad>\", 0)]], dtype=torch.long).to(device)\n",
        "  outputs = []\n",
        "\n",
        "  for _ in range(max_len):\n",
        "    with torch.no_grad():\n",
        "      output, hidden, cell = model.decoder(decoder_input, hidden, cell)\n",
        "      token = output.argmax(2)\n",
        "      token_id = token.item()\n",
        "      outputs.append(token_id)\n",
        "\n",
        "    decoder_input = token\n",
        "\n",
        "    if token_id == tokenizer.word2idx.get(\"<pad>\", 0):\n",
        "      break\n",
        "\n",
        "  return tokenizer.decode(outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISjIj-rVQy5R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5141b6bd-2380-4a1c-e595-35d62e72b956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            " is a of of of of of the of a of the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
          ]
        }
      ],
      "source": [
        "input_text = (\n",
        "    \"A loss function measures how wrong your modelâ€™s predictions are. It uses the difference between the model's prediction and actual outputs. This value is used for updating the model's parameters to decrease loss. This project approaches text summarization as sequence-to-sequence classification, hence Cross-Entropy Loss is chosen. Since it is widely used for multi-class classification tasks.\"\n",
        ")\n",
        "\n",
        "summary = predict(model, simple_tokenizer, input_text)\n",
        "print(\"Summary:\\n\", summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7NQfcbGRceh"
      },
      "source": [
        "to see vocab\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GJ9zUedRfPb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da37fd47-d028-4616-fb26-def6b41528fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: <pad> -> 0\n",
            "1: <unk> -> 1\n",
            "2: the -> 2\n",
            "3: of -> 3\n",
            "4: a -> 4\n",
            "5: and -> 5\n",
            "6: to -> 6\n",
            "7: in -> 7\n",
            "8: we -> 8\n",
            "9: is -> 9\n",
            "10: for -> 10\n",
            "11: that -> 11\n",
            "12: on -> 12\n",
            "13: this -> 13\n",
            "14: with -> 14\n",
            "15: are -> 15\n",
            "16: as -> 16\n",
            "17: by -> 17\n",
            "18: an -> 18\n",
            "19: our -> 19\n",
            "20: from -> 20\n",
            "21: can -> 21\n",
            "22: be -> 22\n",
            "23: which -> 23\n",
            "24: data -> 24\n",
            "25: model -> 25\n",
            "26: it -> 26\n",
            "27: show -> 27\n",
            "28: using -> 28\n",
            "29: network -> 29\n",
            "30: learning -> 30\n",
            "31: or -> 31\n",
            "32: such -> 32\n",
            "33: results -> 33\n",
            "34: these -> 34\n",
            "35: method -> 35\n",
            "36: based -> 36\n",
            "37: have -> 37\n",
            "38: approach -> 38\n",
            "39: at -> 39\n",
            "40: image -> 40\n",
            "41: has -> 41\n",
            "42: propose -> 42\n",
            "43: also -> 43\n",
            "44: algorithm -> 44\n",
            "45: new -> 45\n",
            "46: problem -> 46\n",
            "47: two -> 47\n",
            "48: performance -> 48\n",
            "49: proposed -> 49\n",
            "50: not -> 50\n",
            "51: different -> 51\n",
            "52: more -> 52\n",
            "53: networks -> 53\n",
            "54: both -> 54\n",
            "55: between -> 55\n",
            "56: paper -> 56\n",
            "57: each -> 57\n",
            "58: information -> 58\n",
            "59: their -> 59\n",
            "60: neural -> 60\n",
            "61: present -> 61\n",
            "62: models -> 62\n",
            "63: used -> 63\n",
            "64: @math -> 64\n",
            "65: methods -> 65\n",
            "66: use -> 66\n",
            "67: deep -> 67\n",
            "68: paper, -> 68\n",
            "69: training -> 69\n",
            "70: over -> 70\n",
            "71: than -> 71\n",
            "72: number -> 72\n",
            "73: system -> 73\n",
            "74: its -> 74\n",
            "75: large -> 75\n",
            "76: one -> 76\n",
            "77: into -> 77\n",
            "78: been -> 78\n",
            "79: object -> 79\n",
            "80: where -> 80\n",
            "81: when -> 81\n",
            "82: novel -> 82\n",
            "83: set -> 83\n",
            "84: state-of-the-art -> 84\n",
            "85: only -> 85\n",
            "86: features -> 86\n",
            "87: all -> 87\n",
            "88: algorithms -> 88\n",
            "89: while -> 89\n",
            "90: other -> 90\n",
            "91: but -> 91\n",
            "92: demonstrate -> 92\n",
            "93: time -> 93\n",
            "94: many -> 94\n",
            "95: images -> 95\n",
            "96: how -> 96\n",
            "97: however, -> 97\n",
            "98: most -> 98\n",
            "99: framework -> 99\n",
            "100: existing -> 100\n",
            "101: well -> 101\n"
          ]
        }
      ],
      "source": [
        "for i, (word, idx) in enumerate(word2idx.items()):\n",
        "  print(f\"{i}: {word} -> {idx}\")\n",
        "  if i > 100:\n",
        "    break\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "smsG5AOSYm4e"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eb77bc338edf484ebb52d6d75599ce94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ff236d16fe64fc688273489dc424c2f",
              "IPY_MODEL_77b4fb05a920444ebbce437e56887be2",
              "IPY_MODEL_11236dd54d934503aba3fe65c00d0156"
            ],
            "layout": "IPY_MODEL_0978772c37064cbd8d1e9b15996462b3"
          }
        },
        "3ff236d16fe64fc688273489dc424c2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dade895c526c455b94de9292d6e7f0e3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d2c959e689bc415c9ffe54ed62639de5",
            "value": "Downloadingâ€‡dataâ€‡files:â€‡100%"
          }
        },
        "77b4fb05a920444ebbce437e56887be2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9350c6b9b73412a97c3322786036c1b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3680a50293404c998d38b52f07db58c4",
            "value": 1
          }
        },
        "11236dd54d934503aba3fe65c00d0156": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b440d338250947b1b8ced19c969ad922",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b708cc9c29d34823817f64a91db008db",
            "value": "â€‡1/1â€‡[00:00&lt;00:00,â€‡â€‡8.51it/s]"
          }
        },
        "0978772c37064cbd8d1e9b15996462b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dade895c526c455b94de9292d6e7f0e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2c959e689bc415c9ffe54ed62639de5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9350c6b9b73412a97c3322786036c1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3680a50293404c998d38b52f07db58c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b440d338250947b1b8ced19c969ad922": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b708cc9c29d34823817f64a91db008db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "adaa8eda3bec4f2ebd0e3394f3a10a97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb967a9b8a3a448e8b93027451837618",
              "IPY_MODEL_c7cb1c30b36944f899fad29f4038a387",
              "IPY_MODEL_6a3bc3a3fa2d48a08b101ac5300b1849"
            ],
            "layout": "IPY_MODEL_1cd3b148e51f410e9130fcf441d9c3e1"
          }
        },
        "bb967a9b8a3a448e8b93027451837618": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41079c6a0f9e40bf84b7f1ae88ae5743",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6163fb1190804313abeecdc4454460e5",
            "value": "Extractingâ€‡dataâ€‡files:â€‡100%"
          }
        },
        "c7cb1c30b36944f899fad29f4038a387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d586d525c8324dc59ec0da3965849478",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a4b744e9e0446b1b05d8873abb7731c",
            "value": 1
          }
        },
        "6a3bc3a3fa2d48a08b101ac5300b1849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f56f6a955ad444019c4b8f8f76caf2d0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_634a666a10e1486f80c4802912bb4816",
            "value": "â€‡1/1â€‡[00:00&lt;00:00,â€‡â€‡6.57it/s]"
          }
        },
        "1cd3b148e51f410e9130fcf441d9c3e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41079c6a0f9e40bf84b7f1ae88ae5743": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6163fb1190804313abeecdc4454460e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d586d525c8324dc59ec0da3965849478": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a4b744e9e0446b1b05d8873abb7731c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f56f6a955ad444019c4b8f8f76caf2d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "634a666a10e1486f80c4802912bb4816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}