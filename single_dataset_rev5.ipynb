{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9wqRZsuWbbp"
      },
      "source": [
        "##Data Prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QprJo-jaKpCL"
      },
      "source": [
        "## 2.1 Gather and Explore the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMYCqyAIWd7V"
      },
      "source": [
        "This is for to show what is what !\n",
        "not the actual code or model\n",
        "Get a dataset with (article, summary) pairs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2Cy1VGFw8q8"
      },
      "source": [
        "# Dataset:\n",
        "\n",
        "mounting google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGxNPE4uWqEQ",
        "outputId": "cec0f43e-dd14-4241-b3fb-8df9ce450f88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive is already mounted.\n"
          ]
        }
      ],
      "source": [
        "# mounting the drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "if not os.path.ismount('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Drive is mounted.\")\n",
        "else:\n",
        "    print(\"Drive is already mounted.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jOc6rH5x_As"
      },
      "source": [
        "- as seen in the output of this code block dataset include;\n",
        "\n",
        "  only training split is dowloaded to drive.\n",
        "  all training data's size.\n",
        "  each data contains\n",
        "      - {'aid': ..., 'mid': ..., 'ref_abstract': ..., 'abstract': ...}\n",
        "\n",
        "  aid : article id of the actual paper.\n",
        "\n",
        "  mid : reference id of cited or reference paper.\n",
        "\n",
        "  ref_abstract : abstracts of multiple cited or reference papers. Input documents for the model.\n",
        "\n",
        "  abstract : summary of the paper, ground truth for the model.\n",
        "\n",
        "  article id and their abstract summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535,
          "referenced_widgets": [
            "584feced8f264c54ab7161318da2641d",
            "0d25e4f136404d58b3e59c09e4bb77a5",
            "62c77acfdfcf494bb683a8860364b5e9",
            "b25637e8ca4e46909b4491ada2b335df",
            "6908870dfc9448099f384f3bcd57f762",
            "466cc444d9ee4929a7d0c6e32663f5f8",
            "2af4ca46d09e48c9bd57858ddbeb893f",
            "b7b970521c684e5dbe62ec559dad0ecc",
            "d8d5b2c364e04aaab2c52a01393c6a35",
            "85f0d6b50baf4801bb7b92b445f7940d",
            "122a0263a4474a6ab1319fc10090cdc0",
            "7207044ce63042da88ff868085b1738a",
            "5dc54d364d8640bf89e33f5db6c6559b",
            "93eb49290933420a89f31230c8054be7",
            "701522bd26574314ae2848a22887da10",
            "2b2f02240b9b4451b6636bc0da155c8e",
            "a5e1aa02f81a4071bd97baf7a8d9dd80",
            "197fd28feb3f4a60a93b0e14abc7e458",
            "46ece831c37f4d12bc8d31bc55c85017",
            "ea4c3d86677c40ddb693b19ca05938db",
            "e8caced88a764456b941cff1a0a12331",
            "a8e1fbe6dbca40979c5c6e77759bdc8b"
          ]
        },
        "id": "X8-x4i8x9yfs",
        "outputId": "afc74fdf-7935-452b-b518-75af8c3aee27"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-a487e5ebd9727b25\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-a487e5ebd9727b25/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "584feced8f264c54ab7161318da2641d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7207044ce63042da88ff868085b1738a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-a487e5ebd9727b25/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n",
            "Dataset taken from drive \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "content_of_dataset : {'aid': Value(dtype='string', id=None), 'mid': Value(dtype='string', id=None), 'abstract': Value(dtype='string', id=None), 'ref_abstract': {'mid': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'abstract': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}}\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Total samples: 30369\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "article_id : math9912167\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "reference_paper : 1631980677\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Reference Abstracts (input):\n",
            "  - This note is a sequel to our earlier paper of the same title [4] and describes invariants of rational homology 3-spheres associated to acyclic orthogonal local systems. Our work is in the spirit of\n",
            "the Axelrodâ€“Singer papers [1], generalizes some of their results, and furnishes a new setting for the purely topological implications of their work.\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "  - Recently, Mullins calculated the Casson-Walker invariant of the 2-fold cyclic branched cover of an oriented link in S^3 in terms of its Jones polynomial and its signature, under the assumption that\n",
            "the 2-fold branched cover is a rational homology 3-sphere. Using elementary principles, we provide a similar calculation for the general case. In addition, we calculate the LMO invariant of the p-fold\n",
            "branched cover of twisted knots in S^3 in terms of the Kontsevich integral of the knot.\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Summary\n",
            "  - Author(s): Kuperberg, Greg; Thurston, Dylan P. | Abstract: We give a purely topological definition of the perturbative quantum invariants of links and 3-manifolds associated with Chern-Simons field\n",
            "theory. Our definition is as close as possible to one given by Kontsevich. We will also establish some basic properties of these invariants, in particular that they are universally finite type with\n",
            "respect to algebraically split surgery and with respect to Torelli surgery. Torelli surgery is a mutual generalization of blink surgery of Garoufalidis and Levine and clasper surgery of Habiro.\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import textwrap\n",
        "!pip install datasets==2.0 --quiet\n",
        "import textwrap\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "save_dir = \"/content/drive/MyDrive/00_Bireysel/02_dersler/nlp/project/my_paper/resources\"\n",
        "save_path = os.path.join(save_dir, \"multi_science_dataset.json\")\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "  print(\"Dataset downloading...\")\n",
        "  multi_xscience = load_dataset(\"bigbio/multi_xscience\")\n",
        "  multi_xscience['train'].to_json(save_path)\n",
        "  multi_xscience = multi_xscience[multi_xscience[\"train\"]]\n",
        "  print(\"Dataset downloaded and saved\")\n",
        "else:\n",
        "  multi_xscience_train = load_dataset(\"json\", data_files=save_path, split=\"train\")\n",
        "  print(\"Dataset taken from drive \")\n",
        "\n",
        "\n",
        "print(f'-'*200)\n",
        "content_of_dataset = multi_xscience_train.features\n",
        "print(f\"content_of_dataset : {content_of_dataset}\")\n",
        "print(f'-'*200)\n",
        "\n",
        "#for split in multi_xscience:\n",
        "#    print(f\"Split: {split}\")\n",
        "data = multi_xscience_train\n",
        "num_samples = len(data)\n",
        "print(f\"Total samples: {num_samples}\")\n",
        "\n",
        "print(f'-'*200)\n",
        "random_index = 0\n",
        "example = multi_xscience_train[random_index]\n",
        "article_id = example['aid']\n",
        "print(f\"article_id : {article_id }\")\n",
        "print(f'-'*200)\n",
        "\n",
        "reference_paper = example[\"mid\"]\n",
        "print(f\"reference_paper : {reference_paper}\")\n",
        "print(f'-'*200)\n",
        "\n",
        "# looping through abstract references.\n",
        "print(\"Reference Abstracts (input):\")\n",
        "for ref in example[\"ref_abstract\"][\"abstract\"]:\n",
        "  print(\"  - \" + textwrap.fill(ref, width=200))\n",
        "  print(f'-'*200)\n",
        "\n",
        "print(\"Summary\")\n",
        "print(\"  - \" + textwrap.fill(example[\"abstract\"], width=200))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw5rC8QG92cY"
      },
      "source": [
        "# some visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR4SgWE5GkP_"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qccs2OW962C"
      },
      "source": [
        "\n",
        "- vocabulary\n",
        "  combining all words from source data to create a vocabulary. lowercase all letters and splitting the words.\n",
        "  using word2idx each word is mapped to unique ID.\n",
        "- what is tokenization :\n",
        "  \n",
        "    converting raw text into tokens, after that each token is mapped to an id so that it can be used as input for models.\n",
        "\n",
        "- what are the parameters\n",
        "      max_legth\n",
        "    is the maximum number of tokens in encoded sentence.\n",
        "      <pad>\n",
        "    in case of shorter sentence padding ensures uniform length\n",
        "      <unk>\n",
        "    representing words that are  not on the vocablary.\n",
        "      truncate\n",
        "    cutting the sentence if it exceeds the max length.\n",
        "\n",
        "- input for tokenization and output of tokenization :\n",
        "\n",
        "  normal source text becomes a list of tokens which include their unique id.\n",
        "  If these tokens are decoded, original words can be extracted with paddings(< pad >)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy-upga597dU"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "class SimpleTokenizer:\n",
        "    full_size = 269401\n",
        "    def __init__(self, data, vocab_limit=full_size//2):  #\n",
        "                                        #269401\n",
        "        self.data = data\n",
        "        self.vocab_limit = vocab_limit\n",
        "        self.build_vocab()\n",
        "\n",
        "    def build_vocab(self):\n",
        "        word_counter = Counter()\n",
        "        for item in self.data:\n",
        "            for text in item[\"ref_abstract\"][\"abstract\"]:\n",
        "                word_counter.update(text.lower().split())\n",
        "            word_counter.update(item[\"abstract\"].lower().split())\n",
        "\n",
        "        # Only keep most common words\n",
        "\n",
        "        most_common_words = word_counter.most_common(self.vocab_limit)\n",
        "        self.word2idx = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "        for idx, (word, _) in enumerate(most_common_words, start=2):\n",
        "            self.word2idx[word] = idx\n",
        "\n",
        "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
        "\n",
        "    def encode(self, text, max_length):\n",
        "        tokens = text.lower().split()\n",
        "        token_ids = [\n",
        "            self.word2idx.get(word, self.word2idx[\"<unk>\"]) for word in tokens\n",
        "        ]\n",
        "        if len(token_ids) < max_length:\n",
        "            token_ids += [self.word2idx[\"<pad>\"]] * (max_length - len(token_ids))\n",
        "        else:\n",
        "            token_ids = token_ids[:max_length]\n",
        "        return token_ids\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        return \" \".join(self.idx2word.get(idx, \"<unk>\") for idx in token_ids)\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return len(self.word2idx)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e44y4-hUGK5Q"
      },
      "source": [
        "## dummy showcase\n",
        "\n",
        "using the dummy dataset we can observe the input and output of this process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iM-EeihT-Emo",
        "outputId": "eb322056-418a-428a-bcd9-8613846a8e09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This note is a sequel to our earlier paper of the same title [4] and describes invariants of rational homology 3-spheres associated to acyclic orthogonal local systems. Our work is in the spirit of\n",
            "the Axelrodâ€“Singer papers [1], generalizes some of their results, and furnishes a new setting for the purely topological implications of their work. Recently, Mullins calculated the Casson-Walker\n",
            "invariant of the 2-fold cyclic branched cover of an oriented link in S^3 in terms of its Jones polynomial and its signature, under the assumption that the 2-fold branched cover is a rational homology\n",
            "3-sphere. Using elementary principles, we provide a similar calculation for the general case. In addition, we calculate the LMO invariant of the p-fold branched cover of twisted knots in S^3 in terms\n",
            "of the Kontsevich integral of the knot.\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "encoded : [34, 35, 7, 6, 36, 8, 11, 37, 38, 2, 3, 39, 40, 41, 4, 42, 15, 2, 16, 17, 43, 18, 8, 44, 45, 46, 47, 11, 48, 7, 5, 3, 49, 2, 3, 50, 51, 52, 53, 19, 2, 20, 54, 4, 55, 6, 56, 57, 21, 3, 22, 23, 58, 2, 20, 59, 60, 61, 62, 3, 63, 24, 2, 3, 25, 64, 12, 13, 2, 65, 66, 67, 5, 26, 5, 27, 2, 28, 68, 69, 4, 28, 70, 71, 3, 72, 29, 3, 25, 12, 13, 7, 6, 16, 17, 73, 74, 75, 76, 9, 77, 6, 78, 79, 21, 3, 80, 81, 5, 82, 9, 83, 3, 84, 24, 2, 3, 85, 12, 13, 2, 86, 87, 5, 26, 5, 27, 2, 3, 88, 89, 2, 3, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "this note is a sequel to our earlier paper of the same title [4] and describes invariants of rational homology 3-spheres associated to acyclic orthogonal local systems. our work is in the spirit of\n",
            "the axelrodâ€“singer papers [1], generalizes some of their results, and furnishes a new setting for the purely topological implications of their work. recently, mullins calculated the casson-walker\n",
            "invariant of the 2-fold cyclic branched cover of an oriented link in s^3 in terms of its jones polynomial and its signature, under the assumption that the 2-fold branched cover is a rational homology\n",
            "3-sphere. using elementary principles, we provide a similar calculation for the general case. in addition, we calculate the lmo invariant of the p-fold branched cover of twisted knots in s^3 in terms\n",
            "of the kontsevich integral of the knot. <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
          ]
        }
      ],
      "source": [
        "dummy_train_data = [multi_xscience_train[0]]  # Only 1 sample\n",
        "\n",
        "source_text = \" \".join(dummy_train_data[0][\"ref_abstract\"][\"abstract\"])\n",
        "print(textwrap.fill(source_text, width=200))\n",
        "print(\"-\" * 200)\n",
        "\n",
        "simple_tokenizer = SimpleTokenizer(dummy_train_data)\n",
        "encoded = simple_tokenizer.encode(source_text, max_length = 153)\n",
        "print(f\"encoded : {encoded}\")\n",
        "print(\"-\" * 200)\n",
        "\n",
        "decoded = simple_tokenizer.decode(encoded)\n",
        "print(textwrap.fill(decoded, width=200))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EuvPYY0GvuF"
      },
      "source": [
        "#Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zIHCZJmHxxv"
      },
      "source": [
        "- Encoders :\n",
        "\n",
        "  encoders takes sequence of tokens and turn them into hidden representaion. In this case using embedding layer and Long Short-Term Memory (LSTM) layer.\n",
        "\n",
        "- Parameters :\n",
        "   \n",
        "      vocab_size  \n",
        "  is obtained from the tokenizer class\n",
        "      embed_dim\n",
        "  how many numbers represent each wordÃ§\n",
        "      hidden_dim\n",
        "  size of LSTM's memory\n",
        "      num_layers\n",
        "  number of stacked LSTM layers\n",
        "    \n",
        "\n",
        "\n",
        "- pipeline itself\n",
        "      \n",
        "  1 - embedding_layer\n",
        "  \n",
        "  input : token id shape is (1, 153(max_legth))\n",
        "      embed_dim = 128\n",
        "  output : embedded vectors with shape (1, 153, 128)\n",
        "  1 : number of sequence.\n",
        "  153 : number of tokens\n",
        "  128 : each word become dimensional vector.\n",
        "\n",
        "  2 - LSTM Layer\n",
        "\n",
        "  input : (1, 153, 128)\n",
        "  for each word LSTM outpur (1, 153, 256) 256-dim vector\n",
        "  with two LSTM layers (2, 153, 256)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  Output *****(LOOK AT THERE ! NEED REV)\n",
        "\n",
        "\n",
        "      outputs\n",
        "  LSTM output at each time sterp. (used for attention)\n",
        "\n",
        "  embeddings are located here.\n",
        "\n",
        "      hidden\n",
        "  final hidden state of each LSTM layer. (initializing decoder)\n",
        "    a kind of memory of LSTM\n",
        "      cell\n",
        "  \n",
        "  internal memory of each LSTM(initializing decoder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_2t7-QaGwT3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "    self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
        "    print(f\"self.embedding : {self.embedding}\") # DB - Size of your vocab (136 unique tokens: words + <pad> + <unk>)\n",
        "    print(f\"self.lstm : {self.lstm}\")           # DB - Each word will be represented as a vector of 128 numbers (dense, not one-hot).\n",
        "  def forward(self, input_ids):\n",
        "    embedded = self.embedding(input_ids)               # (batch_size, seq_len, embed_dim)\n",
        "    outputs, (hidden, cell) = self.lstm(embedded)       # (batch_size, seq_len, hidden_dim)\n",
        "    return outputs, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8psJ1YMzGzVx",
        "outputId": "fbdfc2b2-e245-4c83-d969-b00807412931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This note is a sequel to our earlier paper of the same title [4] and describes invariants of rational homology 3-spheres associated to acyclic orthogonal local systems. Our work is in the spirit of\n",
            "the Axelrodâ€“Singer papers [1], generalizes some of their results, and furnishes a new setting for the purely topological implications of their work. Recently, Mullins calculated the Casson-Walker\n",
            "invariant of the 2-fold cyclic branched cover of an oriented link in S^3 in terms of its Jones polynomial and its signature, under the assumption that the 2-fold branched cover is a rational homology\n",
            "3-sphere. Using elementary principles, we provide a similar calculation for the general case. In addition, we calculate the LMO invariant of the p-fold branched cover of twisted knots in S^3 in terms\n",
            "of the Kontsevich integral of the knot.\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "vocab_size : 136\n",
            "self.embedding : Embedding(136, 128)\n",
            "self.lstm : LSTM(128, 256, num_layers=2, batch_first=True)\n",
            "Input tensor shape (to Encoder) : torch.Size([1, 153])\n",
            "Outputs shape (from Encoder)    : torch.Size([1, 153, 256])\n",
            "Hidden state shape              : torch.Size([2, 1, 256])\n",
            "Cell state shape                : torch.Size([2, 1, 256])\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "tensor([[-0.0094,  0.0422, -0.0105,  0.0155, -0.0162,  0.0149,  0.0002,  0.0182,\n",
            "          0.0283,  0.0188],\n",
            "        [-0.0278,  0.0436, -0.0238,  0.0343, -0.0265,  0.0071, -0.0042,  0.0223,\n",
            "          0.0441,  0.0194],\n",
            "        [ 0.0050,  0.0521, -0.0026,  0.0341, -0.0455,  0.0077, -0.0175,  0.0266,\n",
            "          0.0405,  0.0273],\n",
            "        [ 0.0318,  0.0425, -0.0106,  0.0125, -0.0527,  0.0395, -0.0173,  0.0590,\n",
            "          0.0395,  0.0166],\n",
            "        [ 0.0195,  0.0376,  0.0346,  0.0212, -0.0530,  0.0225, -0.0021,  0.0725,\n",
            "          0.0279,  0.0043]], grad_fn=<SliceBackward0>)\n",
            "tensor([[ 1.5824e-01, -8.7981e-02, -4.1276e-02, -6.0082e-02,  2.8479e-01,\n",
            "          1.1260e-04, -6.8997e-02,  1.6895e-01, -1.5890e-01,  1.1202e-01],\n",
            "        [-2.8924e-02,  4.8138e-02, -1.7990e-02,  1.3862e-01, -3.6320e-02,\n",
            "          1.7547e-02,  2.6354e-02,  4.8072e-02,  1.1786e-01,  2.6890e-02]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "tensor([[ 4.1828e-01, -1.8421e-01, -7.7792e-02, -1.7157e-01,  5.9211e-01,\n",
            "          1.5274e-04, -1.9933e-01,  3.0565e-01, -3.3438e-01,  2.8775e-01],\n",
            "        [-5.4099e-02,  1.0144e-01, -3.4146e-02,  2.8632e-01, -7.9760e-02,\n",
            "          3.5372e-02,  5.9910e-02,  9.4067e-02,  2.3079e-01,  5.9022e-02]],\n",
            "       grad_fn=<SliceBackward0>)\n"
          ]
        }
      ],
      "source": [
        "## DUMMY USECASE !\n",
        "import torch\n",
        "import textwrap\n",
        "\n",
        "dummy_train_data = [multi_xscience_train[0]]\n",
        "\n",
        "# creating a dummy dataset\n",
        "source_text = \" \".join(dummy_train_data[0][\"ref_abstract\"][\"abstract\"])\n",
        "print(textwrap.fill(source_text, width=200))\n",
        "print(\"-\" * 200)\n",
        "\n",
        "# using tokenizer\n",
        "simple_tokenizer = SimpleTokenizer(dummy_train_data)\n",
        "\n",
        "encoded = simple_tokenizer.encode(source_text, max_length=153)\n",
        "\n",
        "decoded = simple_tokenizer.decode(encoded)\n",
        "input_tensor = torch.tensor([encoded], dtype=torch.long)  # batch_size = 1\n",
        "\n",
        "# Initializing Encoder\n",
        "vocab_size = simple_tokenizer.get_vocab_size()\n",
        "print(f\"vocab_size : {vocab_size}\")\n",
        "embed_dim = 128\n",
        "hidden_dim = 256\n",
        "num_layers = 2\n",
        "\n",
        "encoder = Encoder(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=embed_dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    num_layers=num_layers\n",
        ")\n",
        "\n",
        "# pass through encoder !\n",
        "outputs, hidden, cell = encoder(input_tensor)\n",
        "\n",
        "# 9. Print what went into encoder and what came out\n",
        "print(f\"Input tensor shape (to Encoder) : {input_tensor.shape}\")     # (batch_size, seq_len)\n",
        "print(f\"Outputs shape (from Encoder)    : {outputs.shape}\")          # (batch_size, seq_len, hidden_dim)\n",
        "print(f\"Hidden state shape              : {hidden.shape}\")           # (num_layers, batch_size, hidden_dim)\n",
        "print(f\"Cell state shape                : {cell.shape}\")             # (num_layers, batch_size, hidden_dim)\n",
        "print(\"-\" * 200)\n",
        "print(outputs[0, :5, :10])  # first 5 tokens, first 10 hidden dimensions\n",
        "print(hidden[:, 0, :10])    # hidden states (all layers), first 10 hidden dims\n",
        "print(cell[:, 0, :10])      # cell states (all layers), first 10 hidden dims\n",
        "\n",
        "# Word \"the\" â†’ [0.12, -0.44, 0.78, ..., 0.01] (a vector of 128 numbers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBepFDGMXI_U"
      },
      "source": [
        "#Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJYNOTziXMY8"
      },
      "source": [
        "In terms of structure decoder and encoder are similar.\n",
        "\n",
        "both use embedding layyers to convert tokens to vectors\n",
        "\n",
        "use LSTM to process sequence of data\n",
        "\n",
        "start with <sos> token then continue one by one.\n",
        "It outputs a sequence of token prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqI8lGV6XLe2"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "    self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
        "    self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "  def forward(self, input_ids, hidden, cell):\n",
        "    embedded = self.embedding(input_ids)  # (batch_size, seq_len, embed_dim)\n",
        "    outputs, (hidden, cell) = self.lstm(embedded, (hidden, cell))  # Use the previous hidden and cell states\n",
        "    predictions = self.fc_out(outputs)  # (batch_size, seq_len, vocab_size)\n",
        "    return predictions, hidden, cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2wjk4iec26H"
      },
      "source": [
        "# seq2seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpyK_t8FdoyN"
      },
      "source": [
        "##\n",
        "\n",
        "encoder converts input to memory\n",
        "\n",
        "decoder create new output from memory(encoder)\n",
        "\n",
        "\n",
        "seq2seq handles one full forward pass!\n",
        "\n",
        "      def forward(self, src, trg, teacher_forcing_ratio=0.5)\n",
        "\n",
        "  forward is the actual method, responsible for the pass through network.\n",
        "  \n",
        "      src\n",
        "  represent the input ensor with shape of batch size and src_seq_len\n",
        "\n",
        "      trg\n",
        "  ground truth output\n",
        "\n",
        "      teacher_forcing_ratio\n",
        "  a float value between 0 and 1, making seq-2-seq models help to decode faster, by using gorund - truth token as the next decoder input during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r_hXSmWf-d2"
      },
      "source": [
        "##flow\n",
        "\n",
        "output of forward method's one pass  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVaoYJitX5Dy"
      },
      "outputs": [],
      "source": [
        "# actual seq2seq class !\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, device):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "\n",
        "  def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "    batch_size = src.shape[0]\n",
        "    trg_len = trg.shape[1]\n",
        "    vocab_size = self.decoder.fc_out.out_features\n",
        "\n",
        "    outputs = torch.zeros(batch_size, trg_len, vocab_size).to(self.device)\n",
        "\n",
        "    encoder_outputs, hidden, cell = self.encoder(src)\n",
        "\n",
        "    input_token = trg[:, 0].unsqueeze(1)  # start with <sos> token (or just the first token)\n",
        "\n",
        "    for t in range(1, trg_len):\n",
        "      output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
        "      outputs[:, t] = output.squeeze(1)\n",
        "\n",
        "      # Decide whether to use teacher forcing\n",
        "      teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "      top1 = output.argmax(2)\n",
        "\n",
        "      input_token = trg[:, t].unsqueeze(1) if teacher_force else top1\n",
        "\n",
        "    return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzmtiTgmgikd"
      },
      "source": [
        "# Dataset Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zniQHTtW0lK"
      },
      "source": [
        "actual dataset class to better use our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azTeuqesgkhb"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self, data, tokenizer, max_input_len=256, max_target_len=128):\n",
        "    self.data = data\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_input_len = max_input_len\n",
        "    self.max_target_len = max_target_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = self.data[idx]\n",
        "\n",
        "    # Prepare source and target texts\n",
        "    source_text = \" \".join(item[\"ref_abstract\"][\"abstract\"])\n",
        "    target_text = item[\"abstract\"]\n",
        "\n",
        "    # Tokenize\n",
        "    source_ids = self.tokenizer.encode(source_text, self.max_input_len)\n",
        "    target_ids = self.tokenizer.encode(target_text, self.max_target_len)\n",
        "\n",
        "    return {\n",
        "      \"input_ids\": torch.tensor(source_ids, dtype=torch.long),\n",
        "      \"labels\": torch.tensor(target_ids, dtype=torch.long)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye5YndLMgYYg"
      },
      "source": [
        "# initilaizing all\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxaBf7VtgaFu",
        "outputId": "c895238c-91ac-4c8b-c019-e6432ba976fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30369\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = MyDataset(multi_xscience_train, simple_tokenizer)\n",
        "# Select subset\n",
        "print(f\"{len(train_dataset)}\")\n",
        "reduced_data = multi_xscience_train.select(range(len(train_dataset)))\n",
        "\n",
        "# Create tokenizer\n",
        "simple_tokenizer = SimpleTokenizer(reduced_data)\n",
        "#simple_tokenizer = SimpleTokenizer(reduced_data, vocab_limit=10000)\n",
        "\n",
        "# Wrap with dataset\n",
        "train_dataset = MyDataset(reduced_data, simple_tokenizer)\n",
        "\n",
        "# Create DataLoader\n",
        "# 128/512/256\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers = 16, pin_memory = True) # 16 YAP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smsG5AOSYm4e"
      },
      "source": [
        "# training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQUQpJTdZddE"
      },
      "source": [
        "      device\n",
        "\n",
        "  if available choose a CPU cuda else runs on CPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O_mLnA4Zp9k"
      },
      "source": [
        "  These are hyperparameters that effect model's performance.\n",
        "\n",
        "      vocab_size\n",
        "      embed_dim = 256\n",
        "\n",
        "  - it is how many numbers represent each word(token)\n",
        "  - higher dimension meaning a richer representation with more memory usage.\n",
        "  - much higher dimensions is not neccesary in small datasets.\n",
        "\n",
        "\n",
        "      hidden_dim = 512\n",
        "\n",
        "  \n",
        "  - size of the hidden state in LSTM.\n",
        "  - it is responsible for how much info LSTM can store and process.\n",
        "  - bigger memory meaning better \"thinking\", yet can be expensive computationaly.\n",
        "  - higher risk of overfitting in small datasets.     \n",
        "      \n",
        "\n",
        "      num_layers = 2\n",
        "\n",
        "- number of stacked LSTM layers.\n",
        "- more layer a better model that can comprehend more complex patterns.\n",
        "- too many layers create diffuculty in training.\n",
        "  \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i76e7gNXc5bZ"
      },
      "source": [
        "  setting up values for encoder, decoder and wrap them into seq2seq model.\n",
        "\n",
        "      encoder = Encoder(vocab_size, embed_dim, hidden_dim, num_layers)\n",
        "      decoder = Decoder(vocab_size, embed_dim, hidden_dim, num_layers)\n",
        "      model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "----------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31J8b7PddEAF"
      },
      "source": [
        "based on how our model learns we can adjust these values.\n",
        "\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "optimizer :\n",
        "  \n",
        "  - controls how the model updates its weights, according to the loss.\n",
        "\n",
        "ADAM - Adaptive Moment Estimation :\n",
        "  - chosen according to popularity amoung ML applications.\n",
        "\n",
        "  - using momentum it updates learning rates adaptively.\n",
        "\n",
        "learning rate :\n",
        "  - chosen according to safe value for ADAM optimizer.  \n",
        "  - important hyperparameter.\n",
        "  - controls how big each update is to the weights.\n",
        "   \n",
        "   *small learning rate = > model learns slow\n",
        "   \n",
        "   *big model skip important details.\n",
        "- - - - - -- - - - - - -\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIv6cpMWfjX8"
      },
      "source": [
        "loss function :\n",
        "\n",
        "- telling the model, how wrong it is. Since we are predicting tokens from vocablary CrossEntropyLoss is good choice.\n",
        "\n",
        "- <pad> tokens are ignored, since they represent padding, not actually meaningfull words.\n",
        "\n",
        "      criterion = nn.CrossEntropyLoss(ignore_index=simple_tokenizer.word2idx[\"<pad>\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPSeWETnfqVV"
      },
      "source": [
        "Epochs :\n",
        "\n",
        "this is where the actual loop is located.\n",
        "1 epoch meaning a full pass through the dataset.\n",
        "      \n",
        "      num_of_epochs\n",
        "\n",
        "decide how many times dataset will go through the model during training.\n",
        "\n",
        "-----------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC_BKx8bgkjU"
      },
      "source": [
        "      for i, batch in enumerate(train_loader):\n",
        "        src = batch[\"input_ids\"].to(device)\n",
        "        trg = batch[\"labels\"].to(device)\n",
        "  \n",
        "iterating in training data in mini-batches.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      input = batch[\"input_ids\"].to(device)\n",
        "      target = batch[\"labels\"].to(device)\n",
        "\n",
        "\n",
        "- src : input, target : target summary as tensors.\n",
        "- these tensors are moved to device(GPU or CPU)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "- clears ol gradients from last step. precent gradient accumulation.\n",
        "\n",
        "\n",
        "      \n",
        "      output = model(inputs, target)\n",
        "- passes the source and target througgh seq2seq model\n",
        "output : is a tensor of predicted word probabilities, also called logits\n",
        "\n",
        "      output_dim = output.shape[-1]    \n",
        "      output = output[:, 1:].reshape(-1, output_dim)\n",
        "      target = target[:, 1:].reshape(-1)\n",
        "\n",
        "- skipping the sos token\n",
        "- flattening predictions and targets for CrossEntropyLoss\n",
        "\n",
        "output - shape => [batch_size * seq_len, vocab_size]\n",
        "\n",
        "target - shape => [batch_size * seq_len]\n",
        "\n",
        "--------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Loy0hLf0k0qU"
      },
      "source": [
        "      loss = criterion(output, target)\n",
        "\n",
        "- comparing model prediction and actual token\n",
        "- how wrong the prediction is\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIPfmqFElVaf"
      },
      "source": [
        "      loss.backward()\n",
        "- backpropagation step, calculate gradients for all parameters and observe how much each is contributed to the loss.\n",
        "- how much we need to adjust each weight for reduced loss.\n",
        "----------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvFSc27Xltz3"
      },
      "source": [
        "      optimizer.step()\n",
        "- updating the model weights according yo previous calculated(loss.backward).\n",
        "\n",
        "- optimizer uses gradients and learning rate to adjust weights.\n",
        "\n",
        "--------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwdgV-YWmFpC"
      },
      "source": [
        "Every 10 batches, the batch number is displayed.\n",
        "the average loss is calculated to observe training performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh3Hpufd2W9c"
      },
      "source": [
        "# training code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-T7eAHiYoD7"
      },
      "outputs": [],
      "source": [
        "# Step 5 - Training\n",
        "train = False\n",
        "if train:\n",
        "\n",
        "  device = torch.device(\"cuda\")\n",
        "\n",
        "  vocab_size = simple_tokenizer.get_vocab_size()\n",
        "  embed_dim = 256\n",
        "  hidden_dim = 256\n",
        "  num_layers = 2\n",
        "\n",
        "  encoder = Encoder(vocab_size, embed_dim, hidden_dim, num_layers)\n",
        "  decoder = Decoder(vocab_size, embed_dim, hidden_dim, num_layers)\n",
        "  model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "  criterion = nn.CrossEntropyLoss(ignore_index=simple_tokenizer.word2idx[\"<pad>\"])\n",
        "\n",
        "  num_of_epochs = 15\n",
        "\n",
        "  for epoch in range(num_of_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "      inputs = batch[\"input_ids\"].to(device)\n",
        "      target = batch[\"labels\"].to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      output = model(inputs, target)\n",
        "\n",
        "      output_dim = output.shape[-1]\n",
        "      output = output[:, 1:].reshape(-1, output_dim)\n",
        "      target = target[:, 1:].reshape(-1)\n",
        "\n",
        "      loss = criterion(output, target)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if i % 10 == 0:\n",
        "        print(f\"  Batch {i}/{len(train_loader)}\")\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_of_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "  import os\n",
        "  number_of_revision = 1\n",
        "  save_path = \"/content/drive/MyDrive/00_Bireysel/02_dersler/nlp/project/my_paper\"\n",
        "  os.makedirs(save_path, exist_ok=True)\n",
        "  save_name = f\"{number_of_revision}_seq2seq_model_5.pth\"\n",
        "  torch.save(model.state_dict(), os.path.join(save_path, save_name))\n",
        "\n",
        "  import json\n",
        "  with open(os.path.join(save_path, \"vocab.json\"), \"w\") as f:\n",
        "      json.dump(simple_tokenizer.word2idx, f)\n",
        "\n",
        "  print(f\"Model and tokenizer saved to {save_path}\")\n",
        "else:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpUtfKLvm4xr"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C905GV27SE05"
      },
      "outputs": [],
      "source": [
        "#310 21 01\n",
        "class SimpleTokenizer:\n",
        "  def __init__(self, word2idx):\n",
        "      self.word2idx = word2idx\n",
        "      self.idx2word = {int(v): k for k, v in word2idx.items()}\n",
        "\n",
        "  def encode(self, text, max_length):\n",
        "      tokens = text.lower().split()\n",
        "      token_ids = [self.word2idx.get(word, self.word2idx[\"<unk>\"]) for word in tokens]\n",
        "      token_ids = token_ids[:max_length]\n",
        "      token_ids += [self.word2idx[\"<pad>\"]] * (max_length - len(token_ids))\n",
        "      return token_ids\n",
        "\n",
        "  def decode(self, token_ids):\n",
        "      return \" \".join([self.idx2word.get(int(idx), \"<unk>\") for idx in token_ids])\n",
        "\n",
        "  def get_vocab_size(self):\n",
        "      return len(self.word2idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAUS6Jow15x0",
        "outputId": "6fb9afd3-a899-4445-c0a1-b6bdce31a3ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "134702\n",
            "self.embedding : Embedding(134702, 256)\n",
            "self.lstm : LSTM(256, 256, num_layers=2, batch_first=True)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(134702, 256)\n",
              "    (lstm): LSTM(256, 256, num_layers=2, batch_first=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(134702, 256)\n",
              "    (lstm): LSTM(256, 256, num_layers=2, batch_first=True)\n",
              "    (fc_out): Linear(in_features=256, out_features=134702, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "with open(\"/content/drive/MyDrive/00_Bireysel/02_dersler/nlp/project/my_paper/vocab.json\", \"r\") as f:\n",
        "    word2idx = json.load(f)\n",
        "\n",
        "simple_tokenizer = SimpleTokenizer(word2idx)\n",
        "# Hyperparameters (must match training)\n",
        "vocab_size = simple_tokenizer.get_vocab_size()\n",
        "print(vocab_size)\n",
        "embed_dim = 256\n",
        "hidden_dim = 256\n",
        "num_layers = 2\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Initialize encoder, decoder, and model\n",
        "encoder = Encoder(vocab_size, embed_dim, hidden_dim, num_layers)\n",
        "decoder = Decoder(vocab_size, embed_dim, hidden_dim, num_layers)\n",
        "model = Seq2Seq(encoder, decoder, device)\n",
        "\n",
        "# Load trained weights\n",
        "model.load_state_dict(torch.load(\n",
        "    \"/content/drive/MyDrive/00_Bireysel/02_dersler/nlp/project/my_paper/1_seq2seq_model_5.pth\",\n",
        "    map_location=\"cpu\"\n",
        "))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5bBI_hMQxqN"
      },
      "outputs": [],
      "source": [
        "def predict(model, tokenizer, input_text, max_len=64):\n",
        "    model.eval()\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "    # Encode input text\n",
        "    input_ids = tokenizer.encode(input_text, max_length=128)\n",
        "    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
        "\n",
        "    # Run encoder\n",
        "    with torch.no_grad():\n",
        "        _, hidden, cell = model.encoder(input_tensor)\n",
        "\n",
        "    # Start with <pad> token or any start token\n",
        "    decoder_input = torch.tensor([[tokenizer.word2idx.get(\"<pad>\", 0)]], dtype=torch.long).to(device)\n",
        "    outputs = []\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(decoder_input, hidden, cell)\n",
        "            token = output.argmax(2)  # greedy decoding\n",
        "            token_id = token.item()\n",
        "            outputs.append(token_id)\n",
        "\n",
        "        decoder_input = token\n",
        "\n",
        "        # Optionally stop at <pad>\n",
        "        if token_id == tokenizer.word2idx.get(\"<pad>\", 0):\n",
        "            break\n",
        "\n",
        "    return tokenizer.decode(outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISjIj-rVQy5R",
        "outputId": "51aec83c-6922-4b2e-b238-0495f3d4d028"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary:\n",
            " and is the task of the the of of of the and of the the of of the the of of the the of of the the of of the the of of the the of of the the of of the the of the the of the the the of the the of the the of the the of the the the of\n"
          ]
        }
      ],
      "source": [
        "input_text = (\n",
        "    \"An unmanned aerial vehicle (UAV) or unmanned aircraft system (UAS), commonly known as a drone, \"\n",
        "    \"is an aircraft with no human pilot, crew, or passengers onboard, but rather is controlled remotely or is autonomous. \"\n",
        "    \"UAVs were originally developed through the twentieth century for military missions too dull, dirty or dangerous for humans, \"\n",
        "    \"and by the twenty-first, they had become essential assets to most militaries. \"\n",
        "    \"As control technologies improved and costs fell, their use expanded to many non-military applications. \"\n",
        "    \"These include aerial photography, area coverage, precision agriculture, forest fire monitoring, river monitoring, \"\n",
        "    \"environmental monitoring, weather observation, policing and surveillance, infrastructure inspections, smuggling, \"\n",
        "    \"product deliveries, entertainment, drone racing, and combat.\"\n",
        ")\n",
        "\n",
        "\n",
        "input_text = (\n",
        "    \"Sequence-to-sequence (seq2seq) is a type of neural network architecture. \"\n",
        "    \"They are able to transform one sequence to another, meaning it can understand the context of the text and act accordingly. \"\n",
        "    \"Using an encoder and decoder pair it can map input to output. After the encoder processes the input and compresses it to the context vector, \"\n",
        "    \"the decoder generates an output sequence, one token at each time step. Embedding layers are used to obtain dense vector representation. \"\n",
        "    \"These vectors are passed through the LSTM layer that capture relationships between words. This state is called the hidden state. \"\n",
        "    \"At the end the decoder receives this hidden state and processes it at each time step.\"\n",
        ")\n",
        "\n",
        "summary = predict(model, simple_tokenizer, input_text)\n",
        "print(\"Summary:\\n\", summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSmB5AMNUJ-F"
      },
      "source": [
        "# loglar\n",
        "model5\n",
        "epoch 15\n",
        "bacth total 950\n",
        "embed_dim 256\n",
        "hidden_dim 256\n",
        "num_layer = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e60g31dklUT"
      },
      "source": [
        "2\n",
        "reduced_data = multi_xscience_train.select(range(30000//2))  # Only first 200\n",
        "\n",
        "simple_tokenizer = SimpleTokenizer(reduced_data)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers = 2, pin_memory = True)\n",
        "\n",
        "embed_dim = 32\n",
        "\n",
        "hidden_dim = 64\n",
        "\n",
        "num_layers = 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7NQfcbGRceh"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GJ9zUedRfPb"
      },
      "outputs": [],
      "source": [
        "example = data[0]  # or another known-good index\n",
        "input_text = \" \".join(example[\"ref_abstract\"][\"abstract\"])\n",
        "print(f\"input text : {input_text}\")\n",
        "summary = predict(model, simple_tokenizer, input_text)\n",
        "print(\"Summary:\\n\", summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a84p5NyFQ60j"
      },
      "source": [
        "# GRB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0DbjV1eVUuD"
      },
      "outputs": [],
      "source": [
        "def predict(model, tokenizer, input_text, max_input_len=100, max_output_len=50):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    input_ids = tokenizer.encode(input_text, max_length=max_input_len)\n",
        "    input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    _, hidden, cell = model.encoder(input_tensor)\n",
        "\n",
        "    output_indices = [tokenizer.word2idx[\"<pad>\"]]\n",
        "    outputs = torch.tensor([output_indices], dtype=torch.long).to(device)\n",
        "\n",
        "    for _ in range(max_output_len):\n",
        "        preds, hidden, cell = model.decoder(outputs[:, -1].unsqueeze(1), hidden, cell)\n",
        "        next_token = preds.argmax(-1)[:, -1].item()\n",
        "        outputs = torch.cat([outputs, torch.tensor([[next_token]]).to(device)], dim=1)\n",
        "        if next_token == tokenizer.word2idx[\"<pad>\"]:\n",
        "            break\n",
        "\n",
        "    return tokenizer.decode(outputs.squeeze().tolist()[1:])  # remove initial <pad>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umbVYgzWWcYL"
      },
      "outputs": [],
      "source": [
        "model = torch.load(\"/content/drive/MyDrive/00_Bireysel/02_dersler/nlp/project/my_paper/1_seq2seq_model_5_ESAS.pth\", map_location=\"cpu\")\n",
        "#model.eval()\n",
        "input_text = \"An unmanned aerial vehicle (UAV) or unmanned aircraft system (UAS), commonly known as a drone, is an aircraft with no human pilot, crew, or passengers onboard, but rather is controlled remotely or is autonomousUAVs were originally developed through the twentieth century for military missions too dull, dirty or dangerous for humans, and by the twenty-first, they had become essential assets to most militaries. As control technologies improved and costs fell, their use expanded to many non-military applications.These include aerial photography, area coverage,precision agriculture, forest fire monitoring,river monitoring, environmental monitoring, weather observation, policing and surveillance, infrastructure inspections, smuggling, product deliveries, entertainment, drone racing, and combat\"\n",
        "summary = predict(model, simple_tokenizer, input_text)\n",
        "\n",
        "print(\"ðŸ”¸ Predicted Summary:\")\n",
        "print(summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vE3UELKUQvwv"
      },
      "outputs": [],
      "source": [
        "def predict(model, tokenizer, input_text, max_len=64):\n",
        "    model.eval()\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "    # Encode input text\n",
        "    input_ids = tokenizer.encode(input_text, max_length=128)\n",
        "    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
        "\n",
        "    # Run encoder\n",
        "    with torch.no_grad():\n",
        "        _, hidden, cell = model.encoder(input_tensor)\n",
        "\n",
        "    # Start with <pad> token or any start token\n",
        "    decoder_input = torch.tensor([[tokenizer.word2idx.get(\"<pad>\", 0)]], dtype=torch.long).to(device)\n",
        "    outputs = []\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(decoder_input, hidden, cell)\n",
        "            token = output.argmax(2)  # greedy decoding\n",
        "            token_id = token.item()\n",
        "            outputs.append(token_id)\n",
        "\n",
        "        decoder_input = token\n",
        "\n",
        "        # Optionally stop at <pad>\n",
        "        if token_id == tokenizer.word2idx.get(\"<pad>\", 0):\n",
        "            break\n",
        "\n",
        "    return tokenizer.decode(outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK7113YrYRsU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "smsG5AOSYm4e"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0d25e4f136404d58b3e59c09e4bb77a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_466cc444d9ee4929a7d0c6e32663f5f8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2af4ca46d09e48c9bd57858ddbeb893f",
            "value": "Downloadingâ€‡dataâ€‡files:â€‡100%"
          }
        },
        "122a0263a4474a6ab1319fc10090cdc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "197fd28feb3f4a60a93b0e14abc7e458": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2af4ca46d09e48c9bd57858ddbeb893f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b2f02240b9b4451b6636bc0da155c8e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "466cc444d9ee4929a7d0c6e32663f5f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46ece831c37f4d12bc8d31bc55c85017": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "584feced8f264c54ab7161318da2641d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d25e4f136404d58b3e59c09e4bb77a5",
              "IPY_MODEL_62c77acfdfcf494bb683a8860364b5e9",
              "IPY_MODEL_b25637e8ca4e46909b4491ada2b335df"
            ],
            "layout": "IPY_MODEL_6908870dfc9448099f384f3bcd57f762"
          }
        },
        "5dc54d364d8640bf89e33f5db6c6559b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5e1aa02f81a4071bd97baf7a8d9dd80",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_197fd28feb3f4a60a93b0e14abc7e458",
            "value": "Extractingâ€‡dataâ€‡files:â€‡100%"
          }
        },
        "62c77acfdfcf494bb683a8860364b5e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7b970521c684e5dbe62ec559dad0ecc",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8d5b2c364e04aaab2c52a01393c6a35",
            "value": 1
          }
        },
        "6908870dfc9448099f384f3bcd57f762": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "701522bd26574314ae2848a22887da10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8caced88a764456b941cff1a0a12331",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a8e1fbe6dbca40979c5c6e77759bdc8b",
            "value": "â€‡1/1â€‡[00:00&lt;00:00,â€‡23.62it/s]"
          }
        },
        "7207044ce63042da88ff868085b1738a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5dc54d364d8640bf89e33f5db6c6559b",
              "IPY_MODEL_93eb49290933420a89f31230c8054be7",
              "IPY_MODEL_701522bd26574314ae2848a22887da10"
            ],
            "layout": "IPY_MODEL_2b2f02240b9b4451b6636bc0da155c8e"
          }
        },
        "85f0d6b50baf4801bb7b92b445f7940d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93eb49290933420a89f31230c8054be7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46ece831c37f4d12bc8d31bc55c85017",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ea4c3d86677c40ddb693b19ca05938db",
            "value": 1
          }
        },
        "a5e1aa02f81a4071bd97baf7a8d9dd80": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8e1fbe6dbca40979c5c6e77759bdc8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b25637e8ca4e46909b4491ada2b335df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85f0d6b50baf4801bb7b92b445f7940d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_122a0263a4474a6ab1319fc10090cdc0",
            "value": "â€‡1/1â€‡[00:00&lt;00:00,â€‡108.08it/s]"
          }
        },
        "b7b970521c684e5dbe62ec559dad0ecc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8d5b2c364e04aaab2c52a01393c6a35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8caced88a764456b941cff1a0a12331": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea4c3d86677c40ddb693b19ca05938db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
